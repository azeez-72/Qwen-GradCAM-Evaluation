{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04FMWUmmnho3"
   },
   "source": [
    "# Baseline 1 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFx7BcP6uwxh"
   },
   "source": [
    "### Initial package installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UZ2hb3tYkd-0",
    "outputId": "a1bc47f7-6938-412a-e6ff-5aaedcce5ab5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting qwen-vl-utils\n",
      "  Downloading qwen_vl_utils-0.0.14-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting av (from qwen-vl-utils)\n",
      "  Downloading av-16.0.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from qwen-vl-utils) (25.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from qwen-vl-utils) (11.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from qwen-vl-utils) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->qwen-vl-utils) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->qwen-vl-utils) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->qwen-vl-utils) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->qwen-vl-utils) (2025.11.12)\n",
      "Downloading qwen_vl_utils-0.0.14-py3-none-any.whl (8.1 kB)\n",
      "Downloading av-16.0.1-cp312-cp312-manylinux_2_28_x86_64.whl (40.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: av, qwen-vl-utils\n",
      "Successfully installed av-16.0.1 qwen-vl-utils-0.0.14\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers accelerate bitsandbytes pillow pandas torch opencv-python einops\n",
    "!pip install qwen-vl-utils\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0A4phUgA52P"
   },
   "source": [
    "### Setup and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rqWTSGGFhYRF",
    "outputId": "a6da6fb2-9272-4c6b-b39d-e86012e65204"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded. Running on cuda with image size (768, 768)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch.backends.cudnn as cudnn\n",
    "import warnings\n",
    "\n",
    "# Clean Environment\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "os.environ['HUGGINGFACE_HUB_FORCE_REDOWNLOAD'] = '1'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "def get_config(\n",
    "    model_id=\"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    seed=520,\n",
    "    image_root_dir=\"/content/drive/MyDrive/VLM/paper_figures/\",\n",
    "):\n",
    "    \"\"\"Returns a dictionary containing all configuration settings, excluding dynamic values.\"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    PANEL_PROMPT = \"\"\"\n",
    "      You are a visual-language expert specialized in analyzing multi-panel scientific figures. Each panel may be labeled with letters (A, B, C, D, etc.) or alphanumeric labels (e.g., 1A, A5, 3B, etc.).\n",
    "\n",
    "      Task\n",
    "      1. Examine the provided figure carefully and identify all visible panel labels (A, B, 1A, A5, etc.).\n",
    "      2. Determine which specific labeled panel best supports the following scientific claim:\n",
    "\n",
    "      Claim:\n",
    "      \"{claim}\"\n",
    "\n",
    "      Instructions\n",
    "      1. Read and interpret each labeled panel visually ‚Äì focus on trends, comparisons, correlations, or experimental results.\n",
    "      2. Identify which panel (by its exact visible label) provides the strongest direct visual evidence supporting the claim.\n",
    "      3. The label can be alphabetic or alphanumeric (e.g., A, B, 5A, A5, C2, etc.).\n",
    "      4. If no panel clearly supports the claim, output Panel: None and briefly explain why.\n",
    "      5. Provide a concise but clear explanation ‚Äì up to 3 lines having max 20 words, focusing on visual reasoning only.\n",
    "\n",
    "      Output Format (must follow exactly)\n",
    "      - Line 1: Panel: <exact visible label or None>\n",
    "      - Line 2: Reason: <first line of reasoning>\n",
    "      - Line 3 (optional): <second line of reasoning>\n",
    "      - Line 4 (optional): <third line of reasoning>\n",
    "\n",
    "      Do NOT include any extra commentary, numbering, markdown, or quotes.\n",
    "\n",
    "      Provide your answer in the exact format above.\n",
    "      \"\"\"\n",
    "\n",
    "    return {\n",
    "        \"MODEL_ID\": model_id,\n",
    "        \"SEED\": seed,\n",
    "        \"DEVICE\": device,\n",
    "        \"IMAGE_ROOT_DIR\": image_root_dir,\n",
    "        \"PANEL_PROMPT\": PANEL_PROMPT,\n",
    "    }\n",
    "\n",
    "def set_seeds(seed):\n",
    "    \"\"\"Sets random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "\n",
    "config = get_config()\n",
    "set_seeds(config[\"SEED\"])\n",
    "print(f\"‚úÖ Configuration loaded. Running on {config['DEVICE']} with image size {config['IMAGE_SIZE']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-qDGRPrA9k1"
   },
   "source": [
    "### Initialize and Load the Qwen Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p36cgu50-ugt"
   },
   "outputs": [],
   "source": [
    "def initialize_model_and_processor(model_id, device):\n",
    "    \"\"\"Loads the Qwen2.5-VL model and processor.\"\"\"\n",
    "    print(f\"üß† Loading model: {model_id}...\")\n",
    "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "        model_id,\n",
    "        dtype=torch.float32,\n",
    "        device_map=\"cuda\"\n",
    "    )\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "    print(\"‚úÖ Model and processor loaded.\")\n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344,
     "referenced_widgets": [
      "718fd6a16952441c82a693d1a3cf1c9b",
      "e8d06c9142cf4e2ead4533125aeb5237",
      "6f26e84da33f498fb4800b7bed133bf0",
      "21504a0703014ebd853c1433198a29f3",
      "6f422c81a6234cc9ad98a911f2daf30f",
      "b19fd9a01c5d4701b5cf72bf39524c3f",
      "f20bca0a11e249d09ff379f4bb7b242f",
      "0523e84431404e28b9fa576680a3a1b7",
      "adba6cdd2f10468f8256bdb73f0cc0ae",
      "3597a63b37ce4dcabcca9e0fd29b0ec1",
      "e780a74a737042588f599cdbfca6683f",
      "c8b89098e8fd4f068fda6689906e067e",
      "45e7b13e99a44b5b978eb0995522c1e3",
      "2f3381dd991c4c31b6e2e7d7781a86b3",
      "940a63b01acb4b9a9aeb180fa31d7c34",
      "82a44a1d63c44398a257f50ecc2fc665",
      "a40da344f41940028363f75cbff7c381",
      "0aef01d4b0ab4bf19f2cc238e4836713",
      "e50a52af66644004bc2632c76ffc907a",
      "7976b8bb7e3542618a0bc8edb1d21237",
      "11832aab8b7a41789504eed26dd1aee6",
      "edf948725ca14076aa2c461bab369f3a",
      "5d1311a7389f4eb69c1a783ac9a260ba",
      "8cec37322aed466ea945e2f7e828ddf2",
      "4346ce55890b41e8876c49eeb5de041c",
      "90c37158a8f94990bb412fd062ba72c3",
      "c4b10eadf1c94faf9213c7cc6704d6a4",
      "da16e55cee4f4e0aa46baf82f6fdab3a",
      "ee588e1bea5949cdb7c2f24b887596e9",
      "652c9270f2f7410bbfd0ebda94c6bdc9",
      "febba6d7dc90491383fd6f394b2cb595",
      "2c3de361ffb246a28491afa070490e74",
      "6b01c4dc2e9e4b9f9ae43db25855595c",
      "4e71e76d02654d17b609a9a11d24e598",
      "7191c791f5c0432e824799f20f22bd2d",
      "82a191e7d8704e30be7dfdf5b2903873",
      "d979c9468a47450b98a67645b3aaeef1",
      "17c976ae552e4999974c3a5f98997a08",
      "7c25575cb3b5428b9e951a0dc195e947",
      "c8439f30e33e4523b85c8760f2425d11",
      "585518250afe4cde99fb63725f9f5460",
      "11c4392df6234d188812af796b229331",
      "cff2bc726d8f4558ba8db8d778c39a06",
      "4a13d6df6aa2408ba6ce4cbde2a587a0",
      "e06b1d96bdc34ef7b29f708162051ab4",
      "b930ce34b01f40d3822b13e09b42f761",
      "5ff6854d63df4e039f48d32572dbc5a1",
      "45e598b2d26340d2a489fa78be5e01fd",
      "6e6f0b62757846c0b7cc02089b88b15c",
      "8e57ca1e349740049b49e87c5d5eba2b",
      "83e54acc2ed646ad9de702cf81e1b6dd",
      "8a72907900864be68e62b872af8c7b98",
      "8f8eb0f6ff424a109f137ff19d8e35c6",
      "7190805c0285427799bdbdcc75b830cd",
      "2834316a2a3649cfaab203833de705f1",
      "1df1d881fd85495da3b05022694d4108",
      "716b2ab2db9a45a1960b121c793156db",
      "45e5cbfad7da45cdb78b60fe246ab25e",
      "6290fda5479e45b0bf839cd6bb96d437",
      "9d538f0eb5654bcb86279635aec38e7b",
      "741f60d149b34e7b9941e90a257a8cb8",
      "2511bfa0765147e5a6bfc33d0c934792",
      "68b411bbff6f40d081fb43820652ab4d",
      "d01a309686f1449abb8d17d325937830",
      "176a248551274dab8bcaadfdfad426cb",
      "6d17c0945c914d5ba2787fc413078882",
      "d8b8b8b4e01d44e19c175043f4727c69",
      "2b640f67f2e3418abd69b48034f5a0a8",
      "0df1dbb62f20488ba88a90ced59b4215",
      "d857dfcc97b74e4ca8dc510fe4e36916",
      "a253f8d12eef42a7bd8e931718cab836",
      "330788e852a54b68825f5381efaa3f0b",
      "895ea973b9294cdb9b7e02c64a0351b6",
      "8d3992c389a84c149ce4807f29af8c78",
      "88a4933d4373408b8e64dfe8b3556553",
      "35fa216be50842ce807f3fee2724274e",
      "0a4590101a104a8897976b7a4503fd20",
      "eeae3c08359e46ccb5fe30dd31cb808a",
      "892c607a814e4dd198dfb5715ed05b93",
      "c08d36ff2a3f4867961a89afea497f99",
      "80e0935fbaf740aaa7995987059e58e4",
      "32581ab7714a4ba698cc0b2b0abbab44",
      "cdb8e9ea6c304652b736301550a1df62",
      "f833e43713dd4f75b1169ca901983c8e",
      "9cbac004567244668d6612fe8d7ea83b",
      "ad501a6cb93540d69c1d2bcffbaa8bab",
      "5a857bc08ea444cba64a083e42a38380",
      "849293e8c21e483992045623a05c45f7"
     ]
    },
    "id": "BcEoTpBw-2Va",
    "outputId": "e198fb3b-c0c0-466c-99db-e35cc4004832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Loading model: Qwen/Qwen2.5-VL-7B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718fd6a16952441c82a693d1a3cf1c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b89098e8fd4f068fda6689906e067e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/216 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1311a7389f4eb69c1a783ac9a260ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e71e76d02654d17b609a9a11d24e598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06b1d96bdc34ef7b29f708162051ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df1d881fd85495da3b05022694d4108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b8b8b4e01d44e19c175043f4727c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeae3c08359e46ccb5fe30dd31cb808a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model and processor loaded.\n"
     ]
    }
   ],
   "source": [
    "model, processor = initialize_model_and_processor(config[\"MODEL_ID\"], config[\"DEVICE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2E0i3n8aBMzD"
   },
   "source": [
    "### Load and Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130,
     "referenced_widgets": [
      "710ce3f2c80b4666bff7057a5bfc7f25",
      "4a41aecb368648a5b7197cdd8703ee4a",
      "44a35248c7c24d219cd1d4c553cf8446",
      "4233a087d9cd47a0baa01cde5aae0a79",
      "6fa7a8e844e14a16a2633a2faec4d1cb",
      "e698dc4f8e1945479c950c798793aff2",
      "b1bd032d862240faae303ca85a7a30da",
      "2cb161559bf24c1b9c3e82e5d298e133",
      "ad0297e242c64f7083f7d5090e0b7d4a",
      "395d6d02c90749cda0cee93fa6ca8191",
      "63453587852c4f5e86c921ec4ff10f7c",
      "0281e6cc13ec4e899049d858645f39bb",
      "f7cf92da77c74a4bb531fd7db72ec297",
      "7fc9d2a049884905b710cab04f26962c",
      "e0b53c9e356244649114a3bdeb6abe8c",
      "b95eeb2cca614449b318fb6c02599454",
      "86e08d01e5934c8797239591c93947ec",
      "ca3d04daea0d4f358d0bf2ece55c9ab0",
      "19a06ba0f2e0457eabc250acce8636f1",
      "308c2fb402fa47668d8a80740c05b50e",
      "b2621f6cdf3149eb9fd2b4960ffcb9a7",
      "c18a09a9dc9b49cf82c759b0dfc7ba2f",
      "ba14b873fec743ebaf27532247106f47",
      "71cb5ffdf40c40e4b6f6e58be42a0625",
      "4629fcb8d12b4c68a44509c30659da64",
      "5564d50450d24a67905bd1ba9774851f",
      "f2264db134b149acaa4e0f5d411b99d3",
      "2f4eb18bce48418f93ef1d7fafc58d41",
      "461b30f88ba745f88eeb0cf867b726ab",
      "b4d48d864ed2404f93881afcb91c70c8",
      "d00c1d2ffc8d48eb921bc111197fb7bf",
      "78351a1b49ba423e87fa64b2345844d0",
      "ac6510cca974462a9db1a90a9f35870d"
     ]
    },
    "id": "VUV-aIaQ-2yW",
    "outputId": "4b2028b9-2be8-44da-daaf-cfed5da616aa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710ce3f2c80b4666bff7057a5bfc7f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0281e6cc13ec4e899049d858645f39bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test_set.jsonl: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba14b873fec743ebaf27532247106f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1515 total rows. Filtered down to: 505 rows\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "from PIL import Image\n",
    "\n",
    "def load_and_filter_data():\n",
    "    \"\"\"Loads the Dataset from Hugging Face and filters rows based on 'support' labels.\"\"\"\n",
    "    ds = load_dataset(\"StonyBrookNLP/MuSciClaims\", split=\"test\")\n",
    "    df = ds.to_pandas()\n",
    "\n",
    "    filtered_df = df[\n",
    "        (df['label_3class'].astype(str).str.lower() == \"support\") &\n",
    "        (df['label_2class'].astype(str).str.lower() == \"support\")\n",
    "    ].copy()\n",
    "\n",
    "    print(f\"Loaded {len(df)} total rows. Filtered down to: {len(filtered_df)} rows\")\n",
    "    return filtered_df\n",
    "\n",
    "data_df = load_and_filter_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4D-plNgBPNc"
   },
   "source": [
    "### Initializing GRAD-CAM for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YlMv8ugNBQsG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "from PIL import Image\n",
    "from einops import rearrange\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3c54ytoxUHjL"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cv2\n",
    "from einops import rearrange\n",
    "\n",
    "class GradCAM():\n",
    "    def __init__(self, model, target_layer, input_token_len, output_ids):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "\n",
    "        self.feature_maps = None\n",
    "        self.gradients = None\n",
    "\n",
    "        self.input_token_len = input_token_len\n",
    "        self.output_ids = output_ids\n",
    "\n",
    "        self._handles = []\n",
    "\n",
    "        # output_ids may be a tensor on CUDA; move to CPU for indexing safely\n",
    "        try:\n",
    "            out_ids_cpu = self.output_ids.detach().cpu() if isinstance(self.output_ids, torch.Tensor) else torch.tensor(self.output_ids)\n",
    "        except Exception:\n",
    "            out_ids_cpu = torch.tensor(self.output_ids)\n",
    "        self.target_ids = out_ids_cpu[0][self.input_token_len:].long()\n",
    "\n",
    "    def save_feature_maps(self, module, input, output):\n",
    "        # output normalize\n",
    "        fmap = output[0] if isinstance(output, (list, tuple)) else output\n",
    "        # expect shape: (batch, seq_len, channels) or (batch, channels, seq_len) depending on implementation\n",
    "        # handle typical case: (batch, seq_len, channels)\n",
    "        if fmap.ndim == 3 and fmap.shape[-1] > fmap.shape[1]:\n",
    "            # if channels appear last, OK (batch, seq_len, channels)\n",
    "            pass\n",
    "        # store\n",
    "        self.feature_maps = fmap\n",
    "        # make sure gradients will be retained for this tensor\n",
    "        try:\n",
    "            self.feature_maps.retain_grad()\n",
    "        except Exception:\n",
    "            # some implementations may not allow retain_grad here; ignore gracefully\n",
    "            pass\n",
    "\n",
    "    def save_gradients(self, module, grad_input, grad_output):\n",
    "        # grad_output may be tuple; pick first\n",
    "        grad_out = grad_output[0] if isinstance(grad_output, (list, tuple)) else grad_output\n",
    "        # detach and store (we'll move to cpu later as needed)\n",
    "        self.gradients = grad_out.detach()\n",
    "\n",
    "    def generate_cam(self, image, inputs, image_len, text_len):\n",
    "        # Register hooks fresh for this call\n",
    "        f_handle = self.target_layer.register_forward_hook(self.save_feature_maps)\n",
    "        b_handle = self.target_layer.register_full_backward_hook(self.save_gradients)\n",
    "        self._handles = [f_handle, b_handle]\n",
    "\n",
    "        try:\n",
    "            self.model.eval()\n",
    "            self.model.zero_grad()\n",
    "\n",
    "            # forward pass (we assume inputs already moved to device)\n",
    "            out = self.model(**inputs)\n",
    "\n",
    "            # compute target logits robustly\n",
    "            # out.logits shape : (batch, seq_len, vocab)\n",
    "            logits = out.logits\n",
    "\n",
    "            # collect the portion corresponding to generated tokens\n",
    "            seq_len = logits.shape[1]\n",
    "            end_idx = min(self.output_ids.shape[1], seq_len) if isinstance(self.output_ids, torch.Tensor) else min(len(self.output_ids[0]), seq_len)\n",
    "            # select token logits corresponding to target tokens and sum them into a scalar\n",
    "            # shape and indexing: logits[0, start:end, vocab_index_of_target_token]\n",
    "            start = self.input_token_len\n",
    "\n",
    "            if start >= end_idx:\n",
    "                raise RuntimeError(f\"Invalid token index range for Grad-CAM: start {start} >= end {end_idx}\")\n",
    "\n",
    "            # build selection: for each position p in [start, end_idx), pick vocab index from target_ids\n",
    "            targ_ids = self.target_ids\n",
    "            if targ_ids.numel() != (end_idx - start):\n",
    "                targ_ids = targ_ids[: (end_idx - start)]\n",
    "\n",
    "            # gather logits per token and sum\n",
    "            token_logits = logits[0, start:end_idx, :]  # shape (L, V)\n",
    "            # ensure target ids on same device\n",
    "            targ_ids = targ_ids.to(token_logits.device)\n",
    "            selected = token_logits[torch.arange(token_logits.shape[0], device=token_logits.device), targ_ids]\n",
    "            target_logits = selected.sum()\n",
    "\n",
    "            # backward to get gradients\n",
    "            target_logits.backward(retain_graph=False)\n",
    "\n",
    "            # at this point hooks should have filled self.feature_maps and self.gradients\n",
    "            if self.feature_maps is None or self.gradients is None:\n",
    "                raise RuntimeError(\"Grad-CAM hooks failed to capture feature maps or gradients\")\n",
    "\n",
    "            # feature_maps expected shape: (batch, seq_len, channels)\n",
    "            fmap = self.feature_maps.detach()\n",
    "            grad = self.gradients.detach()\n",
    "\n",
    "            # If batch dim present, take first element\n",
    "            if fmap.ndim == 3:\n",
    "                fmap = fmap[0]  # (seq_len, channels)\n",
    "            elif fmap.ndim == 4:\n",
    "                # (batch, seq_len, channels, ?) unexpected: try to squeeze\n",
    "                fmap = fmap.reshape(fmap.shape[0], -1)[0]\n",
    "\n",
    "            # If channels last (seq_len, channels) -> rearrange to (channels, h, w)\n",
    "            num_tokens = fmap.shape[0]\n",
    "            side = int(math.sqrt(num_tokens))\n",
    "            if side * side != num_tokens:\n",
    "                possible = [n for n in range(num_tokens) if int(math.sqrt(n))**2 == n]\n",
    "                if possible:\n",
    "                    # pick nearest lower perfect square\n",
    "                    new_num = max([p for p in possible if p <= num_tokens])\n",
    "                    fmap = fmap[:new_num, ...]\n",
    "                    grad = grad[:new_num, ...]\n",
    "                    num_tokens = new_num\n",
    "                    side = int(math.sqrt(num_tokens))\n",
    "                else:\n",
    "                    raise RuntimeError(f\"Cannot reshape feature maps with num_tokens={num_tokens}\")\n",
    "\n",
    "            # now fmap: (num_tokens, channels) and grad: (num_tokens, channels) or (num_tokens, channels)\n",
    "            fmap = rearrange(fmap, '(h w) c -> c h w', h=side, w=side)\n",
    "            grad = rearrange(grad, '(h w) c -> h w c', h=side, w=side)\n",
    "\n",
    "            # rectify gradients, pool across spatial dims for each channel\n",
    "            grad = nn.ReLU()(grad)\n",
    "            pooled_gradients = torch.mean(grad, dim=[0, 1])  # shape (channels,)\n",
    "\n",
    "            # weight activation maps\n",
    "            activation = fmap.to(dtype=torch.float32)\n",
    "            for i in range(activation.size(0)):\n",
    "                activation[i, :, :] *= pooled_gradients[i].to(activation.device)\n",
    "\n",
    "            heatmap = torch.mean(activation, dim=0).cpu().numpy()\n",
    "            heatmap = np.maximum(heatmap, 0)\n",
    "            maxv = heatmap.max() if heatmap.max() > 0 else 1e-6\n",
    "            heatmap = heatmap / maxv\n",
    "\n",
    "            threshold = 0.5\n",
    "            heatmap[heatmap < threshold] = 0\n",
    "\n",
    "            heatmap = cv2.resize(heatmap, (image.size[0], image.size[1]))\n",
    "            heatmap = np.uint8(255 * heatmap)\n",
    "            heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "            original_image = np.array(image)  # ensure numpy array HWC BGR or RGB as expected\n",
    "            if original_image.shape[-1] == 4:\n",
    "                original_image = original_image[..., :3]\n",
    "\n",
    "            superimposed_img = heatmap * 0.4 + original_image\n",
    "            superimposed_img = np.clip(superimposed_img, 0, 255).astype(np.uint8)\n",
    "\n",
    "            return heatmap, superimposed_img\n",
    "\n",
    "        finally:\n",
    "            for h in self._handles:\n",
    "                try:\n",
    "                    h.remove()\n",
    "                except Exception:\n",
    "                    pass\n",
    "            self._handles = []\n",
    "            self.feature_maps = None\n",
    "            self.gradients = None\n",
    "            # clear gradients on model to be safe\n",
    "            try:\n",
    "                self.model.zero_grad()\n",
    "            except Exception:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3R0F-MZMBzfZ"
   },
   "source": [
    "### Generate and store GRADCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dNYvHGDdg9uW"
   },
   "outputs": [],
   "source": [
    "output_dir = \"/content/drive/MyDrive/VLM/Baseline1/overlay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eHptLsTqGpuM",
    "outputId": "a49ca53b-b6b9-44da-9112-9311de6e28e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prediction function ready\n"
     ]
    }
   ],
   "source": [
    "def predict_panel_with_gradcam(image_path, claim, save_dir=None, perform_gradcam=True):\n",
    "    image = Image.open(image_path).resize((384,384), resample=Image.Resampling.BOX)\n",
    "    # image = Image.open(image_path).convert(\"RGB\")\n",
    "    prompt_text = config['PANEL_PROMPT'].format(claim=claim)\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": prompt_text}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    # === Build prompt text ===\n",
    "    # Convert chat messages into a text prompt using the model's chat template\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Preprocess images/videos from the chat messages\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "    # Estimate image token length (Qwen-VL uses 28x28 = 784 pixel patches)\n",
    "    image_len = image_inputs[0].size[0] * image_inputs[0].size[1] // (28 * 28)\n",
    "\n",
    "    # Tokenize and prepare full multimodal input for the model\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "        # min_pixels=config[\"MIN_PIXELS\"],\n",
    "        # max_pixels=config[\"MAX_PIXELS\"]\n",
    "    )\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    # Prepare text-only version to compute text token length (before image_pad tokens)\n",
    "    ptext = processor(\n",
    "        text=[text.split(\"<|image_pad|>\")[0]],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    text_len = ptext['input_ids'].shape[1]\n",
    "\n",
    "    print(f\"image_len: {image_len}, text_len: {text_len}\")\n",
    "\n",
    "    assert len(inputs.input_ids) == 1, inputs  # Ensure batch size = 1\n",
    "\n",
    "    # --- Inference: generating the model output ---\n",
    "\n",
    "    # Run autoregressive generation\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "    # Length of input tokens, used to trim outputs\n",
    "    input_token_len = [len(in_ids) for in_ids in inputs.input_ids][0]\n",
    "\n",
    "    # Remove input tokens from the generated sequence to isolate new output\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    # Decode model output into text\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=False,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    output_text=output_text[0]\n",
    "    print(output_text)\n",
    "\n",
    "    panel_match = re.search(r\"Panel\\s*[:\\-]\\s*([A-Za-z0-9]+)\", output_text)\n",
    "    reason_match = re.search(r\"Reason\\s*[:\\-]\\s*(.+)\", output_text, re.DOTALL)\n",
    "\n",
    "    panel = panel_match.group(1).upper() if panel_match else \"Unknown\"\n",
    "    reason = reason_match.group(1).strip() if reason_match else output_text\n",
    "\n",
    "    # Build combined input (input + generated output) for Grad-CAM or analysis\n",
    "\n",
    "    inputs_out = processor(\n",
    "        text=[text + output_text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs_out = inputs_out.to(\"cuda\")\n",
    "\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if perform_gradcam==False:\n",
    "        return {\"panel\": panel, \"reason\": reason, \"raw_output\": output_text}\n",
    "\n",
    "    try:\n",
    "        target_layer = model.visual.blocks[-1].norm2\n",
    "\n",
    "        gradcam = GradCAM(model, model.visual.blocks[-1].norm2, input_token_len, generated_ids)\n",
    "\n",
    "        print(\"  üé® Generating Grad-CAM...\")\n",
    "\n",
    "        heatmap, superimposed_img = gradcam.generate_cam(image, inputs=inputs_out, image_len=image_len, text_len=text_len)\n",
    "\n",
    "        # visualize\n",
    "        filename = os.path.basename(image_path)\n",
    "        name_only = os.path.splitext(filename)[0]\n",
    "        overlay_path = os.path.join(output_dir, f\"{name_only}_overlay.jpg\")\n",
    "\n",
    "        superimposed_rgb = cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB)\n",
    "        cv2.imwrite(overlay_path, superimposed_rgb)\n",
    "        print(f\"Saved: {overlay_path}\")\n",
    "\n",
    "        return {\"panel\": panel, \"reason\": reason, \"raw_output\": output_text}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Grad-CAM failed: {e}\")\n",
    "        return {\"panel\": panel, \"reason\": reason, \"raw_output\": output_text}\n",
    "\n",
    "print(\"‚úÖ Prediction function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKEjLBlTW6VL"
   },
   "outputs": [],
   "source": [
    "data_df_gradcam = data_df.iloc[[139]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "id": "_HMnrDQmYQSR",
    "outputId": "5192c02f-0c26-42a0-c2de-57c66329336f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "repr_error": "0",
       "type": "dataframe",
       "variable_name": "data_df_gradcam"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-ec1abe48-439b-49b0-99ee-433e9616e1a6\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_claim_id</th>\n",
       "      <th>claim_id</th>\n",
       "      <th>claim_text</th>\n",
       "      <th>label_3class</th>\n",
       "      <th>label_2class</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>associated_figure_filepath</th>\n",
       "      <th>associated_figure_number</th>\n",
       "      <th>associated_figure_panels</th>\n",
       "      <th>caption</th>\n",
       "      <th>claim_from_which_random_figure_is_taken</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>PIIS0092867425002831_10</td>\n",
       "      <td>PIIS0092867425002831_10_support</td>\n",
       "      <td>Within each taxonomic class, gut microbes enco...</td>\n",
       "      <td>SUPPORT</td>\n",
       "      <td>SUPPORT</td>\n",
       "      <td>PIIS0092867425002831</td>\n",
       "      <td>paper_figures/bio_PIIS0092867425002831_images_...</td>\n",
       "      <td>Figure 5</td>\n",
       "      <td>[Panel C]</td>\n",
       "      <td>Taxonomic distribution and genomic coding patt...</td>\n",
       "      <td>None</td>\n",
       "      <td>biology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ec1abe48-439b-49b0-99ee-433e9616e1a6')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-ec1abe48-439b-49b0-99ee-433e9616e1a6 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-ec1abe48-439b-49b0-99ee-433e9616e1a6');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "  <div id=\"id_d4c997d8-dc24-48de-8dc6-76c753a70100\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data_df_gradcam')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_d4c997d8-dc24-48de-8dc6-76c753a70100 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('data_df_gradcam');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "               base_claim_id                         claim_id  \\\n",
       "417  PIIS0092867425002831_10  PIIS0092867425002831_10_support   \n",
       "\n",
       "                                            claim_text label_3class  \\\n",
       "417  Within each taxonomic class, gut microbes enco...      SUPPORT   \n",
       "\n",
       "    label_2class              paper_id  \\\n",
       "417      SUPPORT  PIIS0092867425002831   \n",
       "\n",
       "                            associated_figure_filepath  \\\n",
       "417  paper_figures/bio_PIIS0092867425002831_images_...   \n",
       "\n",
       "    associated_figure_number associated_figure_panels  \\\n",
       "417                 Figure 5                [Panel C]   \n",
       "\n",
       "                                               caption  \\\n",
       "417  Taxonomic distribution and genomic coding patt...   \n",
       "\n",
       "    claim_from_which_random_figure_is_taken   domain  \n",
       "417                                    None  biology  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df_gradcam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wryJuD3CFp4"
   },
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "VFtqsQT0ov_-",
    "outputId": "c63a8930-da79-4e98-be10-8b72f9a552c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîç [1/1] bio_PIIS0092867425002831_images_figure_5.jpg\n",
      "üìã Claim: Within each taxonomic class, gut microbes encode a larger set of CMs compared wi...\n",
      "üéØ Ground Truth: ['Panel C']\n",
      "image_len: 196, text_len: 15\n",
      "Panel: B\n",
      "Reason: Box plots show higher median number of CMs in Firmicutes compared to other classes.\n",
      "None<|im_end|>\n",
      "  üé® Generating Grad-CAM...\n",
      "Saved: /content/drive/MyDrive/VLM/Baseline11/overlay/bio_PIIS0092867425002831_images_figure_5_overlay.jpg\n",
      "‚úÖ Predicted: B\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"imagename\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"bio_PIIS0092867425002831_images_figure_5.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"claim\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Within each taxonomic class, gut microbes encode a larger set of CMs compared with taxonomically similar counterparts.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted_panel\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"B\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ground_truth_panel\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"['Panel C']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reason\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Box plots show higher median number of CMs in Firmicutes compared to other classes.\\nNone<|im_end|>\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "results_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-62590fa4-221b-4fad-9424-1e3446566dab\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imagename</th>\n",
       "      <th>claim</th>\n",
       "      <th>predicted_panel</th>\n",
       "      <th>ground_truth_panel</th>\n",
       "      <th>reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bio_PIIS0092867425002831_images_figure_5.jpg</td>\n",
       "      <td>Within each taxonomic class, gut microbes enco...</td>\n",
       "      <td>B</td>\n",
       "      <td>['Panel C']</td>\n",
       "      <td>Box plots show higher median number of CMs in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-62590fa4-221b-4fad-9424-1e3446566dab')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-62590fa4-221b-4fad-9424-1e3446566dab button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-62590fa4-221b-4fad-9424-1e3446566dab');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "  <div id=\"id_72045686-c3e9-42a9-a55e-769ad8dbec8f\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_72045686-c3e9-42a9-a55e-769ad8dbec8f button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('results_df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                      imagename  \\\n",
       "0  bio_PIIS0092867425002831_images_figure_5.jpg   \n",
       "\n",
       "                                               claim predicted_panel  \\\n",
       "0  Within each taxonomic class, gut microbes enco...               B   \n",
       "\n",
       "  ground_truth_panel                                             reason  \n",
       "0        ['Panel C']  Box plots show higher median number of CMs in ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Performing grad cam on the images now\n",
    "results = []\n",
    "base_image_dir = \"/content/drive/MyDrive/VLM/paper_figures/\"\n",
    "num_to_process = 1  # Start with 3 images\n",
    "\n",
    "for idx, (i, row) in enumerate(data_df_gradcam.head(num_to_process).iterrows()):\n",
    "    img_name = str(row[\"associated_figure_filepath\"].split('/')[-1]).strip()\n",
    "    claim = str(row[\"claim_text\"]).strip()\n",
    "    img_path = base_image_dir + img_name\n",
    "    ass_panel = str(row.get(\"associated_figure_panels\", \"\")).strip()\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üîç [{idx+1}/{num_to_process}] {img_name}\")\n",
    "    print(f\"üìã Claim: {claim[:80]}...\")\n",
    "    print(f\"üéØ Ground Truth: {ass_panel}\")\n",
    "\n",
    "    out = predict_panel_with_gradcam(img_path, claim, perform_gradcam=True)\n",
    "\n",
    "    print(f\"‚úÖ Predicted: {out['panel']}\")\n",
    "\n",
    "    results.append({\n",
    "        \"imagename\": img_name,\n",
    "        \"claim\": claim,\n",
    "        \"predicted_panel\": out[\"panel\"],\n",
    "        \"ground_truth_panel\": ass_panel,\n",
    "        \"reason\": out[\"reason\"]\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
